{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1596/1596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 82ms/step - accuracy: 0.9501 - loss: 0.1222 - val_accuracy: 0.9943 - val_loss: 0.0517\n",
      "Epoch 2/5\n",
      "\u001b[1m1596/1596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 81ms/step - accuracy: 0.9940 - loss: 0.0494 - val_accuracy: 0.9943 - val_loss: 0.0497\n",
      "Epoch 3/5\n",
      "\u001b[1m1596/1596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 81ms/step - accuracy: 0.9931 - loss: 0.0434 - val_accuracy: 0.9919 - val_loss: 0.0492\n",
      "Epoch 4/5\n",
      "\u001b[1m1596/1596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 81ms/step - accuracy: 0.9880 - loss: 0.0385 - val_accuracy: 0.9488 - val_loss: 0.0511\n",
      "Epoch 5/5\n",
      "\u001b[1m1596/1596\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 82ms/step - accuracy: 0.9697 - loss: 0.0343 - val_accuracy: 0.9042 - val_loss: 0.0537\n",
      "Epoch 5: early stopping\n",
      "Test Accuracy: 0.9014\n",
      "\u001b[1m998/998\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 13ms/step\n",
      "[[2.89549828e-01 2.30289856e-03 1.03545794e-02 2.79455096e-03\n",
      "  5.00323288e-02 1.25059355e-02]\n",
      " [4.51938104e-04 2.12289760e-05 2.86873779e-04 7.47806698e-05\n",
      "  1.46587467e-04 3.61629973e-05]\n",
      " [3.88244027e-03 1.47711515e-04 2.68220855e-03 2.58731860e-04\n",
      "  8.74669291e-04 3.52622301e-04]\n",
      " ...\n",
      " [4.38674266e-04 4.91849278e-06 2.42765091e-04 1.11323170e-05\n",
      "  8.80817461e-05 2.57605643e-05]\n",
      " [9.84950125e-01 1.32902535e-02 2.50825226e-01 2.39933073e-03\n",
      "  3.41739804e-01 5.83890080e-03]\n",
      " [6.92011847e-04 1.95415159e-05 9.67865752e-04 3.20311665e-05\n",
      "  2.72519654e-04 1.15222079e-04]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, SpatialDropout1D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = 'D:/git/ToxicCommentClassification/train.csv'  # Change to the actual path\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Function to clean the text\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Lowercase the text\n",
    "    text = re.sub(r'\\r\\n', ' ', text)  # Replace new line characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove non-alphabetic characters\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "    return text\n",
    "\n",
    "# Clean the comment_text column\n",
    "data['comment_text'] = data['comment_text'].apply(clean_text)\n",
    "\n",
    "# Prepare the data for training\n",
    "X = data['comment_text'].values\n",
    "y = data[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad sequences to ensure uniform input size\n",
    "max_length = 150\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_length)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_length)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=20000, output_dim=128))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(6, activation='sigmoid'))  # 6 output neurons for multilabel classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2, verbose=1)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_pad, y_train, epochs=5, batch_size=64, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test_pad, y_test, verbose=0)\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test_pad)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for toxic:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98     28859\n",
      "           1       0.87      0.71      0.78      3056\n",
      "\n",
      "    accuracy                           0.96     31915\n",
      "   macro avg       0.92      0.85      0.88     31915\n",
      "weighted avg       0.96      0.96      0.96     31915\n",
      "\n",
      "\n",
      "\n",
      "Classification report for severe_toxic:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00     31594\n",
      "           1       0.53      0.25      0.34       321\n",
      "\n",
      "    accuracy                           0.99     31915\n",
      "   macro avg       0.76      0.63      0.67     31915\n",
      "weighted avg       0.99      0.99      0.99     31915\n",
      "\n",
      "\n",
      "\n",
      "Classification report for obscene:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99     30200\n",
      "           1       0.89      0.69      0.78      1715\n",
      "\n",
      "    accuracy                           0.98     31915\n",
      "   macro avg       0.94      0.84      0.88     31915\n",
      "weighted avg       0.98      0.98      0.98     31915\n",
      "\n",
      "\n",
      "\n",
      "Classification report for threat:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     31841\n",
      "           1       0.61      0.30      0.40        74\n",
      "\n",
      "    accuracy                           1.00     31915\n",
      "   macro avg       0.80      0.65      0.70     31915\n",
      "weighted avg       1.00      1.00      1.00     31915\n",
      "\n",
      "\n",
      "\n",
      "Classification report for insult:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99     30301\n",
      "           1       0.80      0.59      0.68      1614\n",
      "\n",
      "    accuracy                           0.97     31915\n",
      "   macro avg       0.89      0.79      0.83     31915\n",
      "weighted avg       0.97      0.97      0.97     31915\n",
      "\n",
      "\n",
      "\n",
      "Classification report for identity_hate:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00     31621\n",
      "           1       0.72      0.27      0.39       294\n",
      "\n",
      "    accuracy                           0.99     31915\n",
      "   macro avg       0.86      0.63      0.69     31915\n",
      "weighted avg       0.99      0.99      0.99     31915\n",
      "\n",
      "\n",
      "\n",
      "Overall Accuracy: 0.9199\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Load the CSV file\n",
    "\n",
    "\n",
    "# Convert text data to TF-IDF features\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=20000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Train the SVM model\n",
    "svm = LinearSVC()\n",
    "multi_target_svm = MultiOutputClassifier(svm, n_jobs=-1)\n",
    "multi_target_svm.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = multi_target_svm.predict(X_test_tfidf)\n",
    "\n",
    "# Print classification report\n",
    "for i, column in enumerate(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']):\n",
    "    print(f\"Classification report for {column}:\\n\")\n",
    "    print(classification_report(y_test[:, i], y_pred[:, i]))\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Calculate and print overall accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Overall Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for toxic:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97     28859\n",
      "           1       0.94      0.53      0.68      3056\n",
      "\n",
      "    accuracy                           0.95     31915\n",
      "   macro avg       0.94      0.76      0.82     31915\n",
      "weighted avg       0.95      0.95      0.95     31915\n",
      "\n",
      "\n",
      "\n",
      "Classification report for severe_toxic:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99     31594\n",
      "           1       0.46      0.06      0.10       321\n",
      "\n",
      "    accuracy                           0.99     31915\n",
      "   macro avg       0.73      0.53      0.55     31915\n",
      "weighted avg       0.99      0.99      0.99     31915\n",
      "\n",
      "\n",
      "\n",
      "Classification report for obscene:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99     30200\n",
      "           1       0.92      0.61      0.73      1715\n",
      "\n",
      "    accuracy                           0.98     31915\n",
      "   macro avg       0.95      0.80      0.86     31915\n",
      "weighted avg       0.97      0.98      0.97     31915\n",
      "\n",
      "\n",
      "\n",
      "Classification report for threat:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     31841\n",
      "           1       0.57      0.05      0.10        74\n",
      "\n",
      "    accuracy                           1.00     31915\n",
      "   macro avg       0.78      0.53      0.55     31915\n",
      "weighted avg       1.00      1.00      1.00     31915\n",
      "\n",
      "\n",
      "\n",
      "Classification report for insult:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98     30301\n",
      "           1       0.84      0.42      0.56      1614\n",
      "\n",
      "    accuracy                           0.97     31915\n",
      "   macro avg       0.90      0.71      0.77     31915\n",
      "weighted avg       0.96      0.97      0.96     31915\n",
      "\n",
      "\n",
      "\n",
      "Classification report for identity_hate:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00     31621\n",
      "           1       0.84      0.09      0.17       294\n",
      "\n",
      "    accuracy                           0.99     31915\n",
      "   macro avg       0.92      0.55      0.58     31915\n",
      "weighted avg       0.99      0.99      0.99     31915\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train the Random Forest model\n",
    "random_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "multi_target_forest = MultiOutputClassifier(random_forest, n_jobs=-1)\n",
    "multi_target_forest.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = multi_target_forest.predict(X_test_tfidf)\n",
    "\n",
    "# Print classification report\n",
    "for i, column in enumerate(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']):\n",
    "    print(f\"Classification report for {column}:\\n\")\n",
    "    print(classification_report(y_test[:, i], y_pred[:, i]))\n",
    "    print(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
